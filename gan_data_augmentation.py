# -*- coding: utf-8 -*-
"""GAN_Data_augmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G9fZEuQ90WZCjH2B40eeSjics14QhPVo

#MNSIT GAN for EEG
Alter MNIST GAN for EEG data
"""

import tensorflow as tf
!pip install imageio
!pip install git+https://github.com/tensorflow/docs

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow.keras import layers
import time
import scipy.io
from IPython import display

"""Upload EEG dataset (just 1 trial for now)"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Graduate_Research/EEG_Intention_Estimation/GANs/
target_tmp = scipy.io.loadmat('B0101T.mat')
data = target_tmp['data']
label = target_tmp['label']
# get the data and label
# data - (samples, channels, trials)  (1000, 3, 120)
# label -  (label, 1)

# reshape data into 120, 3, 1000
data = np.transpose(data, (2, 1, 0))  # (trials, channels, samples )
label = np.squeeze(np.transpose(label - 1))
label = tf.convert_to_tensor(label.reshape(-1, 1).astype('float32'))

data = data.reshape(120, 3, 1000, 1).astype('float32')
data = data/5  # I eyeballed the normalization on this one  to test model

BUFFER_SIZE = 120
BATCH_SIZE = 120

#batch and shuffle data
train_dataset = tf.data.Dataset.from_tensor_slices((data, label)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

"""# Model Inputs for Generator and Discriminator

#Generator
"""

def make_generator_model(num_classes, embedding_dim = 50):
    noise_input = tf.keras.Input(shape= (100,), name='noise_input')
    label_input = tf.keras.Input(shape=(1,), name = 'label_input')

    label_embedding = layers.Embedding(input_dim= num_classes, output_dim= embedding_dim)(label_input)
    label_embedding = layers.Flatten()(label_embedding)

    combined_input = layers.Concatenate()([noise_input, label_embedding])
    model = tf.keras.Sequential(name = 'conditional_generator')

    # Start with a Dense layer to project the noise vector to a high-dimensional space
    model.add(layers.Dense(3 * 125 * 128, use_bias=False, input_shape=(100 + embedding_dim,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Reshape((3,125, 128)))

    # Reshape to a 3D tensor to start the convolutional layers
    model.add(layers.Reshape((3, 125, 128)))  # Shape: (3, 125, 128)

    # First Conv2DTranspose layer to increase width
    model.add(layers.Conv2DTranspose(64, (3, 5), strides=(1, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    # Second Conv2DTranspose layer to further increase width
    model.add(layers.Conv2DTranspose(32, (3, 5), strides=(1, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    # Third Conv2DTranspose layer to fine-tune width closer to 1000
    model.add(layers.Conv2DTranspose(16, (3, 5), strides=(1, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    # Final Conv2DTranspose layer to reach the exact output shape of (3, 1000, 1)
    model.add(layers.Conv2DTranspose(1, (3, 7), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))

    conditional_generator = tf.keras.Model([noise_input, label_input], model(combined_input), name = 'conditional_generator')


    return conditional_generator

num_classes = 2
model = make_generator_model(num_classes)
model.summary()

"""#Dsicriminator"""

def make_discriminator_model(num_classes):
    image_input = tf.keras.Input(shape=(3, 1000, 1), name = 'image_input')
    label_input = tf.keras.Input(shape=(1,), name='label_input')

    label_embedding = layers.Embedding(input_dim=num_classes, output_dim=np.prod(image_input.shape[1:]))(label_input)
    label_embedding = layers.Flatten()(label_embedding)
    label_embedding = layers.Reshape((3, 1000, 1))(label_embedding)

    combined_input = layers.Concatenate()([image_input, label_embedding])

    model = tf.keras.Sequential(name='conditional_discriminator')

    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[3, 1000, 2]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    conditional_discriminator = tf.keras.Model([image_input, label_input], model(combined_input), name='conditional_discriminator')

    return conditional_discriminator
num_classes = 2
model = make_discriminator_model(num_classes)
model.summary()

discriminator = make_discriminator_model(num_classes)
generator = make_generator_model(num_classes, embedding_dim = 50)

#noise = tf.random.normal([1, 100])
#label = tf.random.uniform([1, 1], minval=0, maxval=num_classes, dtype=tf.int32)
#generated_image = generator([noise, label], training=False)
#decision = discriminator([generated_image, label], training=False)
#print (decision)

# This method returns a helper function to compute cross entropy loss
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
  # how well can the discriminator distinguish real from fake
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 100
noise_dim = 100
num_examples_to_generate = 1

# You will reuse this seed overtime (so it's easier)
# to visualize progress in the animated GIF)
seed = tf.random.normal([num_examples_to_generate, noise_dim])
seed2 = tf.random.uniform([num_examples_to_generate, 1], minval=0, maxval=num_classes, dtype=tf.int32)

# Notice the use of `tf.function`
# This annotation causes the function to be "compiled".  This is overly complicated and I will change it later on
@tf.function
def train_step(images, labels):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])
    generated_labels = tf.random.uniform([BATCH_SIZE, 1], minval = 0, maxval = num_classes, dtype=tf.int32)

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator([noise, generated_labels], training=True)

      real_output = discriminator([images, label], training=True)
      fake_output = discriminator([generated_images, generated_labels], training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)



    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

def train(dataset, epochs):
  gen_losses = []
  disc_losses = []
  for epoch in range(epochs):
    start = time.time()

    for image_batch, label_batch in dataset:
      g_loss, d_loss = train_step(image_batch, label_batch)
      gen_losses.append(g_loss)
      disc_losses.append(d_loss)

    # Produce images for the GIF as you go
    display.clear_output(wait=True)
   # generate_and_save_images(generator,
   #                         epoch + 1,
   #                         image_batch,label_batch)
    predictions = generator([seed, seed2], training = False)
    print(predictions.shape)
    # plot the generator prediction
    #t = np.linspace(0,4,1000)
    #fig, axs = plt.subplots(3, 1)
    #fig.suptitle('Actively Training Output')
    #axs[0].plot(t, predictions[1, 0, :, 0])
    #axs[0].set_title('Channel 1')
    #axs[1].plot(t, predictions[1, 1, :, 0])
    #axs[1].set_title('Channel 2')
    #axs[2].plot(t, predictions[1, 2, :, 0])
    #axs[2].set_title('Channel 3')
    #fig.supxlabel('Time')
    #fig.supylabel('micro Volts')

    # Plot the losses
    plt.figure(figsize=(10, 5))
    plt.plot(gen_losses, label='Generator Loss')
    plt.plot(disc_losses, label='Discriminator Loss')
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Generator and Discriminator Loss Over Time')
    plt.legend()
    plt.show()

    # Save the model every 15 epochs
    if (epoch + 1) % 15 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # Generate after the final epoch
  #display.clear_output(wait=True)
  #generate_and_save_images(generator,
   #                        epochs,
   #                        seed)

def generate_and_save_images(model, epoch, test_input, test_label):
  # Notice `training` is set to False.
  # This is so all layers run in inference mode (batchnorm).
  predictions = model(test_input, test_label, training=False)

  fig = plt.figure(figsize=(4, 4))

  for i in range(predictions.shape[0]):

      t = np.linspace(0,4,1000)
      fig, axs = plt.subplots(3, 1)
      fig.suptitle('Actively Training Output')
      axs[0].plot(t, predictions[i, 0, :, 0] * 5)
      axs[0].set_title('Channel 1')
      axs[1].plot(t, predictions[i, 1, :, 0] * 5)
      axs[1].set_title('Channel 2')
      axs[2].plot(t, predictions[i, 2, :, 0] * 5)
      axs[2].set_title('Channel 3')
      fig.supxlabel('Time')
      fig.supylabel('micro Volts')

  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  plt.show()

train(train_dataset, EPOCHS)