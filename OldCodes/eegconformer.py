# -*- coding: utf-8 -*-
"""EEGConformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1duCFUF2VYI4nChUNiDOSQ2Nh3426m5Xu

# Conformer

This code is using brain decode and is based off of responses from ChatGpt. Alterations made as necessary to get it to work.

https://braindecode.org/stable/auto_examples/model_building/plot_how_train_test_and_tune.html

https://github.com/eeyhsong/EEG-Conformer/tree/main?tab=readme-ov-file
"""


from braindecode.models import EEGConformer
import numpy as np
import scipy.io
import tensorflow as tf
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch
import torch.optim as optim
import sklearn

from skorch.callbacks import LRScheduler
from torch.nn import Module
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import time

"""# Variables

"""
start_time = time.time()
classes = list(range(2))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

lr = 0.00001
weight_decay = 0.001
batch_size = 128
n_epochs = 400

num_created = 3240
filename_left = 'Conditional_GAN_Data/data_left' + str(num_created) + '.csv'
filename_right = 'Conditional_GAN_Data/data_right' +str(num_created) + '.csv'

#parameters 120, 3, 1000
input_window_samples  = 1000
n_channels = 3
n_outputs = 2

""" Load Augmented Data """
aug_data_left = np.loadtxt(filename_left, delimiter=',')
aug_data_left = aug_data_left.reshape(num_created, 3, 1000)
aug_labels_left = np.zeros(num_created)

aug_data_right = np.loadtxt(filename_right, delimiter=',')
aug_data_right = aug_data_right.reshape(num_created, 3, 1000)
aug_labels_right = np.ones(num_created)

"""# Training Data Load in"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/CS230/GAN/EEG_2B_Data/
data_training =  np.empty((1000, 3, 3240)) # empty np array that concatenates all of the trials
label_training = []  # np.empty((3240,1))
list_of_files_training = ['B0901T.mat', 'B0902T.mat', 'B0903T.mat', 'B0801T.mat', 'B0802T.mat', 'B0803T.mat','B0701T.mat', 'B0702T.mat', 'B0703T.mat',
                 'B0601T.mat', 'B0602T.mat', 'B0603T.mat', 'B0501T.mat', 'B0502T.mat', 'B0503T.mat', 'B0401T.mat', 'B0402T.mat', 'B0403T.mat',
                'B0301T.mat', 'B0302T.mat', 'B0303T.mat', 'B0201T.mat', 'B0202T.mat', 'B0203T.mat', 'B0101T.mat', 'B0102T.mat', 'B0103T.mat']

for idx in range(len(list_of_files_training)):
  target_tmp = scipy.io.loadmat(list_of_files_training[idx])
  data_tmp = target_tmp['data']
  label_tmp = target_tmp['label']
  data_training[:,:, idx*120:(idx+1)*120] = data_tmp
  for i in range(len(label_tmp)):
    if label_tmp[i] == 1.0:
      label_training.append(0)
    else:
      label_training.append(1)


data_training = np.transpose(data_training, (2, 1, 0))  # (trials, channels, ms of data )
labels_training = np.array(label_training)

"""#Load Test Data"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/CS230/GAN/EEG_2B_Data/
data_test =  np.empty((1000, 3, 2160)) # empty np array that concatenates all of the trials
label_test = [] #np.empty((3240,1))
list_of_files_test = ['B0104E.mat', 'B0105E.mat', 'B0204E.mat', 'B0205E.mat','B0304E.mat', 'B0305E.mat','B0404E.mat', 'B0405E.mat',
                          'B0504E.mat', 'B0505E.mat','B0604E.mat', 'B0605E.mat','B0704E.mat', 'B0705E.mat','B0804E.mat', 'B0805E.mat',
                          'B0904E.mat', 'B0905E.mat',]

for idx in range(len(list_of_files_test)):
  target_tmp = scipy.io.loadmat(list_of_files_test[idx])
  data_tmp = target_tmp['data']
  label_tmp = target_tmp['label']
  data_test[:,:, idx*120:(idx+1)*120] = data_tmp
  for i in range(len(label_tmp)):
    if label_tmp[i] == 1.0:
      label_test.append(0)
    else:
      label_test.append(1)


data_test = np.transpose(data_test, (2, 1, 0))  # (trials, channels, ms of data )
labels_test = np.array(label_test)

"""#Normalize Data between 0 and 1"""

# normalize the data to be between [0, 1]

def NormalizeData(data):
  mins = []
  maxes = []
  for i in range(data.shape[0]):
    for j in range(data.shape[1]):
      mins.append(np.min(data[i, j, :]))
      maxes.append(np.max(data[i, j, :]))
  max_data = np.average(maxes)
  min_data = np.average(mins)

  for i in range(data.shape[0]):
    for j in range(data.shape[1]):
      data[i, j, :] = (data[i, j, :] - min_data) / (max_data - min_data)
  return data

"""#Create Dataset"""

class EEGDataSet(Dataset):
  def __init__(self, data, labels, transform=None):
    self.data = data
    self.labels = labels
    self.transform=transform

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index):
    data = self.data[index]
    label = self.labels[index]

    if self.transform:
      data = self.transform(data)

    return data, label

"""#Load Training Data"""
training_data = NormalizeData(data_training)

all_training_data = np.concatenate((data_training, aug_data_right, aug_data_left), axis =0)
all_training_labels = np.concatenate((labels_training, aug_labels_right, aug_labels_left), axis=0)

training_dataset = EEGDataSet(all_training_data, all_training_labels)
#data_loader_training = DataLoader(training_dataset, batch_size=256, shuffle=True)

"""#Load Test Data"""

test_data = NormalizeData(data_test)
test_dataset = EEGDataSet(test_data, labels_test)
#data_loader_test = DataLoader(test_data_set, batch_size=256, shuffle=False)

"""# Load Conformer"""

model = EEGConformer(n_channels=n_channels, n_outputs=n_outputs, input_window_samples=input_window_samples)
model = model.to(device)
"""# Train the Model

Epoch Training
"""

from tqdm import tqdm

def train_one_epoch(
    dataloader: DataLoader, model: Module, loss_fn, optimizer, scheduler:LRScheduler,
    spoch: int, device, print_batch_stats=True
):
  model.train()
  train_loss, correct = 0, 0

  progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),
                      disable= not print_batch_stats)

  for batch_idx, (X, y) in progress_bar:
    X, y = X.to(device).float(), y.to(device)
    optimizer.zero_grad()
    pred = model(X)
    y = y.long()
    loss = loss_fn(pred, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    train_loss += loss.item()
    correct += (pred.argmax(1) == y).sum().item()

    if print_batch_stats:
            progress_bar.set_description(
                f"Epoch {epoch}/{n_epochs}, "
                f"Batch {batch_idx + 1}/{len(dataloader)}, "
                f"Loss: {loss.item():.6f}"
            )
  scheduler.step()
  correct /= len(dataloader.dataset)
  return train_loss / len(dataloader), correct

"""# Test the Model"""

@torch.no_grad()
def test_model(
    dataloader: DataLoader, model: Module, loss_fn, print_batch_stats=True
):

    size = len(dataloader.dataset)
    n_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0

    if print_batch_stats:
      progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))
    else:
      progress_bar = enumerate(dataloader)

    for batch_idx, (X, y) in progress_bar:
      X, y = X.to(device).float(), y.to(device)
      pred = model(X)
      y = y.long()
      batch_loss = loss_fn(pred, y).item()

      test_loss += batch_loss
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()

      #if print_batch_stats:
      #      progress_bar.set_description(
      #          f"Batch {batch_idx + 1}/{len(dataloader)}, "
      #          f"Loss: {batch_loss:.6f}"
      #      )

    test_loss /= n_batches
    correct /= size

    #print(
    #    f"Test Accuracy: {100 * correct:.1f}%, Test Loss: {test_loss:.6f}\n"
    #)
    return test_loss, correct

"""# Define Optimizer, loss, Instatiate data into DataLoader"""

optimizer = torch.optim.AdamW(model.parameters(),
                              lr=lr, weight_decay=weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs -1)
loss_fn = torch.nn.NLLLoss()

train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)

"""# Run the Model"""

train_losses = []
train_accuracies = []
test_accuracies = []
for epoch in range(1, n_epochs +1):
  print(f"Epoch {epoch}/{n_epochs} :", end="")

  train_loss, train_accuracy = train_one_epoch(
        train_loader, model, loss_fn, optimizer, scheduler, epoch, device,
    )
  test_loss, test_accuracy = test_model(test_loader, model, loss_fn)

  train_losses.append(train_loss)
  train_accuracies.append(train_accuracy)
  test_accuracies.append(test_accuracy)

plt.figure(1)
plt.plot(train_losses, label="Train Loss")
plt.plot(train_accuracies, label="Train Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.title('Conformer Loss and Accuracy ')
plt.show()

plt.figure(2)
plt.plot(test_accuracies)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Test Accuracy after each training Epoch')
plt.show()



end_time = time.time()
elapsed_time = end_time - start_time
print(f"Test Accuracy: {test_accuracy},"
f"Test Loss: {test_loss},")
print(f"Execution Time: {elapsed_time}")

 # print(
 #       f"Train Accuracy: {100 * train_accuracy:.2f}%, "
  #      f"Average Train Loss: {train_loss:.6f}, "
  #      f"Test Accuracy: {100 * test_accuracy:.1f}%, "
  #      f"Average Test Loss: {test_loss:.6f}\n"
  #  )
